#!/usr/bin/env python
# Backs up installers, extras, etc. for all your GOG.com purchases.
#
# Copyright (C) 2011  Evan Powers
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

__author__ = 'Evan Powers'
__version__ = '0.1'
__url__ = 'https://github.com/evanpowers/gog-backup'

import sys, os, contextlib, pprint, hashlib
import cookielib, urllib, urllib2, urlparse
import xml.etree.ElementTree
import threading, Queue, time

import html5lib  # http://code.google.com/p/html5lib/

CONCURRENCY = 6
BREADTHFIRST = False
PATHMAP = '.gog.pathmap.py'
COOKIES = '.gog.cookies'
MANIFEST = '.gog.games.py'
VALIDATED = '.gog.valid.py'

LOGIN = 'https://www.gog.com/en/login'
SHELF = 'https://www.gog.com/en/myaccount/shelf'
LIST = 'https://www.gog.com/en/myaccount/list'
THUMB = 'http://www.gog.com'

pathmap = {}
cookiejar = cookielib.LWPCookieJar(COOKIES)
cookieproc = urllib2.HTTPCookieProcessor(cookiejar)
opener = urllib2.build_opener(cookieproc)
treebuilder = html5lib.treebuilders.getTreeBuilder('etree')
parser = html5lib.HTMLParser(tree=treebuilder, namespaceHTMLElements=False)

useragent = 'gog-backup/%s (%s)' % (__version__, __url__)
opener.addheaders = [('User-agent', useragent)]

class AttrDict(dict):
    def __init__(self, **kw):
        self.update(kw)
    def __getattr__(self, key):
        return self[key]
    def __setattr__(self, key, val):
        self[key] = val

def request(url, args=None, range=None, hide=False):
    if not hide:
        print '>', url
    if args is not None:
        args = urllib.urlencode(args)
    req = urllib2.Request(url, data=args)
    if range is not None:
        req.add_header('Range', 'bytes=%d-%d' % range)
    return contextlib.closing(opener.open(req))

def authenticated():
    for cookie in cookiejar:
        if cookie.name == 'cu':
            return True
    return False

def load_attrdicts(fn):
    try:
        with open(fn, 'r') as r:
            ad = r.read().replace('{', 'AttrDict(**{').replace('}', '})')
        return eval(ad)
    except IOError:
        return AttrDict()

def locate(g, f):
    path = os.path.join(pathmap.get(g.key, g.key), f.name)
    if not os.path.isfile(path) and os.path.isfile(f.name):
        return f.name
    return path

def md5map(f, path):
    print '#', path
    H = hashlib.md5()
    part = []
    with open(path, 'rb') as r:
        for start, end, md5 in f.part:
            h = hashlib.md5()
            n = end + 1 - start
            r.seek(start)
            p = r.read(n)
            H.update(p)
            h.update(p)
            if len(p) != n or md5 != h.hexdigest():
                part.append((start, end))
    eq = f.md5 == H.hexdigest()
    if (eq and part) or (not eq and not part):
        # this actually happens with one of my games
        print 'WARNING: error in chunk XML?! GOG.com must have a bug somewhere.'
        print '    %s %s' % (f.md5, H.hexdigest())
        print '    %r' % (part,)
    return part

def compare(valid):
    # compare what we have to the manifest
    games = load_attrdicts(MANIFEST)
    missing, corrupt = [], []
    for g in games:
        for f in g.setup + g.extra:
            k = g.key, f.name
            u = (f.size, f.md5) if hasattr(f, 'md5') else f.size
            v = valid.get(k)
            if not v or v.uniq != u:
                v = AttrDict(key=k, uniq=u, need=[(0, f.size-1)])
                valid[k] = v
            elif not v.need:
                continue  # cache says we have this file

            # try to find a local copy
            path = locate(g, f)
            if not os.path.isfile(path):
                missing.append((g, f))
                continue

            # validate the file we have
            sz = os.path.getsize(path)
            if hasattr(f, 'part'):
                v.need = md5map(f, path)
            elif sz < f.size:
                # assume partial content isn't corrupted...
                v.need = [(sz, f.size-1)]
            elif sz > f.size:
                # ...but extra means total corruption
                v.need = [(0, f.size-1)]
                v.drop = True
            else:
                v.need = []
            if v.need:
                corrupt.append((g, f))

    # write the validity cache to disk
    with open(VALIDATED, 'w') as w:
        print >>w, '# %d files' % len(valid)
        pprint.pprint([valid[k] for k in sorted(valid)], width=100, stream=w)

    return valid, games, missing, corrupt

def megs(b):
    return '%.1fM' % (b / 1000.0**2)

def needed(valid, games, missing, corrupt):
    # print a simple report
    def need(g, f):
        v = valid[(g.key, f.name)]
        return sum(e + 1 - s for s, e in v.need)
    for txt, lst in (('~corrupt', corrupt), ('-missing', missing)):
        mb = megs(sum(need(g, f) for g, f in lst))
        print '%d files are %s (%s):' % (len(lst), txt[1:], mb)
        for g, f in lst:
            pcent = 100.0 * need(g, f) / f.size
            print txt[0], f.name,
            if pcent < 100:
                print '\t(%s, %d%%)' % (megs(need(g, f)), pcent),
            print

def open_notrunc(name, bufsize=4*1024):
    fd = os.open(name, os.O_WRONLY | os.O_CREAT, 0666)
    return os.fdopen(fd, 'wb', bufsize)

def cmd_login(email, passwd):
    with request(LOGIN, args={'log_email': email,
                              'log_password': passwd}) as page:
        assert page.code == 200
    assert authenticated()
    cookiejar.save()

def cmd_manifest():
    assert authenticated()
    games = {}

    # parse game list and available files for each from list page
    with request(LIST) as page:
        assert page.code == 200
        etree = parser.parse(page)
    for game in etree.findall(".//div[@class='tab_1_row']"):
        gamecard = game.find(".//div[@class='tab_1_title']/a")
        g = AttrDict()
        g.key = gamecard.attrib['href'].split('/')[-1]
        g.title = gamecard.text
        g.thumb = THUMB + game.find(".//img[@src]").attrib['src']
        g.setup, g.extra = [], []
        for row in game.findall(".//div[@class='sh_o_i_row']"):
            f = AttrDict()
            f.href = row.attrib.get('onclick', '')
            if not 'download/file' in f.href:
                f.href = row.find(".//a").attrib['href']
                g.setup.append(f)
            else:
                f.href = f.href[f.href.index("'")+1 : -1]
                f.desc = row.find(".//div[@class='sh_o_i_text']/span").text
                g.extra.append(f)
        games[g.key] = g

    # match games to covers on the shelf page
    with request(SHELF) as page:
        assert page.code == 200
        etree = parser.parse(page)
    for game in etree.findall(".//div[@class='shelf_item_h']"):
        gamecard = game.find(".//div[@class='shelf_ov_tab_2_title']/a")
        if gamecard is not None:
            g = games[gamecard.attrib['href'].split('/')[-1]]
            g.cover = THUMB + game.find(".//img[@src]").attrib['src']

    # request a zero-length range from each file to determine
    # - the total size (from the Content-Range header)
    # - the target file name (from the post-redirect URL)
    # - the chunk MD5s for setup files (from the corresponding XML)
    for g in games.values():
        for i, f in enumerate(g.setup + g.extra):
            with request(f.href, range=(0, 0)) as page:
                f.size = int(page.headers['Content-Range'].split('/')[-1])
                f.name = urlparse.urlparse(page.geturl()).path.split('/')[-1]
                if i < len(g.setup):
                    f.url = page.geturl()
        for f in g.setup:
            with request(f.url.replace('?', '.xml?'), hide=True) as page:
                etree = xml.etree.ElementTree.parse(page).getroot()
            del f['url']
            assert f.name == etree.attrib['name']
            assert f.size == int(etree.attrib['total_size'])
            f.md5 = etree.attrib['md5']
            f.part = [(int(chunk.attrib['from']),
                       int(chunk.attrib['to']),
                       chunk.text)
                      for chunk in etree.findall(".//chunk[@method='md5']")]
            assert len(f.part) == int(etree.attrib['chunks'])
            f.part.sort()

    # write all this meta-data to disk
    with open(MANIFEST, 'w') as w:
        print >>w, '# %d games' % len(games)
        pprint.pprint(games.values(), width=100, stream=w)

def cmd_compare():
    # start the comparison from scratch
    needed(*compare({}))

def cmd_update():
    # compare only what's uncached
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    needed(*compare(valid))

def cmd_fetch():
    # start an incremental comparison
    assert authenticated()
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    _, games, _, _ = compare(valid)
    sizes, rates, errors = {}, {}, {}

    # build a list of work items
    work = Queue.PriorityQueue()
    i = -sys.maxint
    for g in games:
        for f in g.setup + g.extra:
            v = valid[(g.key, f.name)]
            path = locate(g, f)
            dn = os.path.dirname(path)
            if dn and not os.path.isdir(dn):
                os.makedirs(dn)
            if os.path.isfile(path) and v.get('drop'):
                os.remove(path)
            i += 1
            for j, (start, end) in enumerate(v.need, -sys.maxint):
                # BREADTHFIRST: first chunk of all files, then second chunk...
                # else: all chunks of first file, then of second file...
                # ...after all chunks, extras in ascending size order
                if hasattr(f, 'part'):
                    prio = j if BREADTHFIRST else i
                else:
                    prio = end
                args = f, start, end, path
                work.put((prio, args))
                sz = end + 1 - start
                sizes[path] = sz + sizes.get(path, 0)

    # work item I/O loop
    def ioloop(tid, path, page, out):
        sz, t0 = True, time.time()
        while sz:
            buf = page.read(4*1024)
            t = time.time()
            out.write(buf)
            sz, dt, t0 = len(buf), t - t0, t
            with lock:
                sizes[path] -= sz
                rates.setdefault(path, []).append((tid, (sz, dt)))

    # worker thread main loop
    def worker():
        tid = threading.current_thread().ident
        while not work.empty():
            _, (f, start, end, path) = work.get()
            try:
                with open_notrunc(path, 4*1024) as out:
                    out.seek(start)
                    se = start, end
                    with request(f.href, range=se, hide=True) as page:
                        hdr = page.headers['Content-Range'].split()[-1]
                        assert hdr == '%d-%d/%d' % (start, end, f.size)
                        assert out.tell() == start
                        ioloop(tid, path, page, out)
                        assert out.tell() == end + 1
            except IOError, e:
                with lock:
                    print >>sys.stderr, '!', path
                    errors.setdefault(path, []).append(e)
            work.task_done()

    # detailed progress report
    def progress():
        with lock:
            left = sum(sizes.values())
            print
            for path, flowrates in sorted(rates.items()):
                flows = {}
                for tid, (sz, t) in flowrates:
                    szs, ts = flows.get(tid, (0, 0))
                    flows[tid] = sz + szs, t + ts
                bps = sum(szs/ts for szs, ts in flows.values())
                print '%10s %8.1fK/s %2dx  %s' % \
                    (megs(sizes[path]), bps / 1000.0, len(flows), path)
            print megs(left), 'remaining'
            rates.clear()

    # process work items with a thread pool
    lock = threading.Lock()
    pool = []
    for i in range(CONCURRENCY):
        t = threading.Thread(target=worker)
        t.daemon = True
        t.start()
        pool.append(t)
    try:
        while any(t.is_alive() for t in pool):
            progress()
            time.sleep(1)
    except:
        with lock:
            if errors:
                print >>sys.stderr, len(errors), 'files had errors:'
            for path in errors:
                print >>sys.stderr, '!', path
                for e in errors[path]:
                    print >>sys.stderr, '\t', e
        raise

def main(args):
    try:
        cookiejar.load()
    except IOError:
        pass

    try:
        with open(PATHMAP, 'rU') as p:
            for line in p:
                k, v = line[:-1].split(None, 1)
                pathmap[k] = v
    except IOError:
        pass

    globals()['cmd_' + args[0]](*args[1:])

    return 0

if __name__ == '__main__':
    sys.exit(main(sys.argv[1:]))
